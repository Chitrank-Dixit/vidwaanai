import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from typing import List, Dict, Any


def calculate_confidence_score(
    question_embedding: List[float],
    retrieved_verses: List[Dict[str, Any]],
    generated_answer: str,
) -> Dict[str, Any]:
    """
    Calculate confidence score for RAG answer based on multiple factors.

    Factors:
    1. Semantic Similarity (40%): Average cosine similarity between question and retrieved verses.
    2. Relevance Ranking (30%): Score based on the rank of retrieved verses.
    3. Answer-to-Evidence Match (20%): Token overlap between answer and retrieved text.
    4. Token Overlap (10%): Raw token overlap percentage.

    Args:
        question_embedding: Embedding vector of the user's question.
        retrieved_verses: List of retrieved verse dictionaries. Each should have 'embedding' and 'text'/'translation'.
        generated_answer: The answer generated by the LLM.

    Returns:
        Dict containing:
            - score: float (0-100)
            - level: str (High, Good, Fair, Low)
            - warning: str (optional warning message)
            - breakdown: Dict (individual component scores)
    """
    if not retrieved_verses:
        return {
            "score": 0.0,
            "level": "Low",
            "warning": "No verses retrieved.",
            "breakdown": {},
        }

    # Extract embeddings and texts
    # Assuming verse['embedding'] is available. If not, we might need to handle it.
    # The current implementation of EmbeddingManager returns lists, so we expect lists here.
    # However, the DB retrieval might return string representations or we might need to parse them.
    # For now, let's assume they are available as lists or numpy arrays in the verse dict.
    # If 'embedding' is missing, we can't calculate similarity fully, so we'll skip or default.

    retrieved_embeddings = []
    retrieved_texts = []

    for verse in retrieved_verses:
        # Text
        text = verse.get("translation") or verse.get("text") or ""
        retrieved_texts.append(text)

        # Embedding
        emb = verse.get("embedding")
        if emb:
            if isinstance(emb, str):
                # Parse string representation if needed (e.g. "[0.1, 0.2]")
                import json

                try:
                    emb = json.loads(emb)
                except (json.JSONDecodeError, TypeError):
                    emb = []  # Fail gracefully
            retrieved_embeddings.append(emb)

    # 1. Semantic Similarity (40%)
    if retrieved_embeddings and question_embedding:
        try:
            # Ensure 2D array for sklearn
            verse_matrix = np.array(retrieved_embeddings)
            if len(verse_matrix.shape) == 1:
                # If single embedding, reshape
                verse_matrix = verse_matrix.reshape(1, -1)

            q_vec = np.array(question_embedding).reshape(1, -1)

            similarities = cosine_similarity(q_vec, verse_matrix)[0]
            avg_similarity = np.mean(similarities)
            similarity_score = avg_similarity * 100
        except Exception as e:
            print(f"Error calculating similarity: {e}")
            similarity_score = 0
    else:
        similarity_score = 0

    # 2. Relevance Ranking (30%)
    # Higher ranked verses = higher relevance.
    # Formula: Average of (1 / (rank + 1))
    relevance_scores = [1 / (i + 1) for i in range(len(retrieved_verses))]
    avg_relevance = np.mean(relevance_scores) * 100
    relevance_score = avg_relevance

    # 3. Answer-to-Evidence Match (20%) & 4. Token Overlap (10%)
    # Normalize texts
    answer_tokens = set(generated_answer.lower().split())
    evidence_text = " ".join(retrieved_texts).lower()
    evidence_tokens = set(evidence_text.split())

    if answer_tokens:
        overlap_count = len(answer_tokens.intersection(evidence_tokens))
        overlap_ratio = overlap_count / len(answer_tokens)
        match_score = overlap_ratio * 100
        token_overlap_score = (
            overlap_ratio * 100
        )  # Using same metric for now as per user logic
    else:
        match_score = 0
        token_overlap_score = 0

    # Weighted Sum
    # Weights: Sim(0.4), Rank(0.3), Match(0.2), Overlap(0.1)
    confidence = (
        (similarity_score * 0.40)
        + (relevance_score * 0.30)
        + (match_score * 0.20)
        + (token_overlap_score * 0.10)
    )

    confidence = min(100.0, max(0.0, confidence))

    # Determine Level
    if confidence >= 80:
        level = "High"
        warning = None
    elif confidence >= 60:
        level = "Good"
        warning = None
    elif confidence >= 40:
        level = "Fair"
        warning = (
            "This answer has moderate uncertainty. Please verify with retrieved verses."
        )
    else:
        level = "Low"
        warning = "Warning: This answer has LOW confidence. The retrieved verses may not be directly relevant."

    return {
        "score": round(confidence, 1),
        "level": level,
        "warning": warning,
        "breakdown": {
            "similarity": round(similarity_score, 1),
            "relevance": round(relevance_score, 1),
            "match": round(match_score, 1),
            "overlap": round(token_overlap_score, 1),
        },
    }
